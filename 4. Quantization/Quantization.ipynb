{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model quantization\n",
        "Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).\n",
        "\n",
        "Reducing the number of bits means the resulting model requires less memory storage, consumes less energy (in theory), and operations like matrix multiplication can be performed much faster with integer arithmetic.\n",
        "\n",
        "In this notebook, we will load a fine tuned IndoBERT model for sentiment analysis task in PyTorch and ONNX format then quantize both models. Finally, we will demonstrate the performance and model size of the quantized PyTorch and ONNX model."
      ],
      "metadata": {
        "id": "Cbl-xKiFKlhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install onnxruntime onnx transformers optimum"
      ],
      "metadata": {
        "id": "kggfdtikzChB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O88FIMKozEYB",
        "outputId": "984892d4-1781-4ddc-876c-bcf2c8c395bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-Zwkbh3mtYqE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "\n",
        "from pathlib import Path\n",
        "import timeit\n",
        "import onnx\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "import onnxruntime as ort\n",
        "from onnxruntime import InferenceSession\n",
        "from onnxruntime.transformers.optimizer import optimize_model\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch"
      ],
      "metadata": {
        "id": "qSxtxKnWMGnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and quantize model (PyTorch)\n",
        "We load the fine-tuned model, and quantize it with PyTorch's dynamic quantization. Finally, we show the model size comparison between full precision and quantized model."
      ],
      "metadata": {
        "id": "5g2Ptt7wLqL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "indobert_path = Path(\"/content/drive/MyDrive/Models/indobert\")\n",
        "model = BertForSequenceClassification.from_pretrained(indobert_path).to(\"cpu\")"
      ],
      "metadata": {
        "id": "dbuLTC-yuGsh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize model\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")"
      ],
      "metadata": {
        "id": "g5Sylf3qyuio"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"/content/temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"/content/temp.p\")/(1024*1024))\n",
        "    os.remove('temp.p')\n",
        "\n",
        "print_size_of_model(model)\n",
        "print_size_of_model(quantized_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqWKHK5EuK7Q",
        "outputId": "ad5d79ca-c3ec-4844-aab8-d5ea803ea334"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (MB): 474.7809534072876\n",
            "Size (MB): 230.1415147781372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the performance of PyTorch quantization"
      ],
      "metadata": {
        "id": "VIrJqbpXMNom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "  i2w = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
        "\n",
        "  # Load data test\n",
        "  test_dataset_path = \"/content/test_preprocess.tsv\"\n",
        "  df_test = pd.read_table(test_dataset_path, header=None)\n",
        "  df_test.rename(columns={0: \"text\", 1: \"label\"}, inplace=True)\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained(indobert_path)\n",
        "\n",
        "  def infer(text):\n",
        "    inputs = tokenizer.encode(text)\n",
        "    inputs = torch.LongTensor(inputs).view(1, -1).to(model.device)\n",
        "\n",
        "    logits = model(inputs)[0]\n",
        "    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "    return i2w[label]\n",
        "\n",
        "  df_test['pred'] = df_test['text'].apply(infer)\n",
        "  acc = accuracy_score(df_test['label'], df_test['pred'])\n",
        "  pre = precision_score(df_test['label'], df_test['pred'], average=\"macro\")\n",
        "  rec = recall_score(df_test['label'], df_test['pred'], average=\"macro\")\n",
        "  f1 = f1_score(df_test['label'], df_test['pred'], average=\"macro\")\n",
        "\n",
        "  return {\"accuracy\": acc,\n",
        "          \"precision\": pre,\n",
        "          \"recall\": rec,\n",
        "          \"f1\": f1}\n",
        "\n",
        "def eval_time(model):\n",
        "  start = time.time()\n",
        "  result = evaluate(model)\n",
        "  print(f\"\"\"\n",
        "        Accuracy:{result['accuracy']}\n",
        "        Precision:{result['precision']}\n",
        "        Recall: {result['recall']}\n",
        "        F1-Score: {result['f1']}\n",
        "        \"\"\")\n",
        "  end = time.time()\n",
        "  print(f\"Evaluation time: {end-start}\\n\")"
      ],
      "metadata": {
        "id": "Is3q-1sh3gfB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Full Model\")\n",
        "eval_time(model)\n",
        "print(\"Quantized Model\")\n",
        "eval_time(quantized_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5WTwOnX6WJR",
        "outputId": "e6d8193b-ee0c-4adb-deaf-2f12b8310a9a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Model\n",
            "\n",
            "        Accuracy:0.916\n",
            "        Precision:0.915580183764327\n",
            "        Recall: 0.875811280223045\n",
            "        F1-Score: 0.8905121652109605\n",
            "        \n",
            "Evaluation time: 102.87095475196838\n",
            "\n",
            "Quantized Model\n",
            "\n",
            "        Accuracy:0.912\n",
            "        Precision:0.906692476448574\n",
            "        Recall: 0.8726061520179167\n",
            "        F1-Score: 0.8854390502105584\n",
            "        \n",
            "Evaluation time: 48.859798431396484\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ONNX\n",
        "We will load the ONNX model that has already been optimized."
      ],
      "metadata": {
        "id": "AanZoRO0MZ48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and quantize ONNX model\n",
        "We will call onnxruntime.quantization.quantize to apply quantization on the BERT model. It supports dynamic quantization with IntegerOps and static quantization with QLinearOps. For activation ONNXRuntime supports only uint8 format for now, and for weight ONNXRuntime supports both int8 and uint8 format.\n",
        "\n",
        "We apply dynamic quantization for BERT model and use int8 for weight."
      ],
      "metadata": {
        "id": "oAeLI2NZMgeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_onnx_model(onnx_model_path, quantized_model_path):\n",
        "    onnx_opt_model = onnx.load(onnx_model_path)\n",
        "    quantize_dynamic(onnx_model_path,\n",
        "                     quantized_model_path,\n",
        "                     weight_type=QuantType.QInt8)\n",
        "\n",
        "onnx_path = Path(\"/content/drive/MyDrive/Models/indobert-onnx/optimized.onnx\")\n",
        "quantize_onnx_path = Path(\"/content/drive/MyDrive/Models/indobert-onnx/quantized_optimized.onnx\")\n",
        "quantize_onnx_model(onnx_path, quantize_onnx_path)"
      ],
      "metadata": {
        "id": "p7jCJisbyJMg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ONNX full precision model size (MB):', os.path.getsize(onnx_path)/(1024*1024))\n",
        "print('ONNX quantized model size (MB):', os.path.getsize(quantize_onnx_path)/(1024*1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDm_UQ1N8CGL",
        "outputId": "07966235-cee3-4916-a504-5e8bc6dcf5bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX full precision model size (MB): 474.74583435058594\n",
            "ONNX quantized model size (MB): 119.11387634277344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate ONNX quantization performance"
      ],
      "metadata": {
        "id": "CwWFrUFeM0nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_onnx(session):\n",
        "  i2w = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
        "\n",
        "  # Load data test\n",
        "  test_dataset_path = \"/content/test_preprocess.tsv\"\n",
        "  df_test = pd.read_table(test_dataset_path, header=None)\n",
        "  df_test.rename(columns={0: \"text\", 1: \"label\"}, inplace=True)\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained(indobert_path)\n",
        "\n",
        "  def infer(text):\n",
        "    inputs = tokenizer([text])\n",
        "    inputs_onnx = dict(\n",
        "        input_ids=np.array(inputs[\"input_ids\"]).astype(\"int64\"),\n",
        "        attention_mask=np.array(inputs[\"attention_mask\"]).astype(\"int64\"),\n",
        "        token_type_ids=np.array(inputs[\"token_type_ids\"]).astype(\"int64\")\n",
        "    )\n",
        "\n",
        "    logits = session.run(None, input_feed=inputs_onnx)[0]\n",
        "    label = torch.topk(torch.from_numpy(logits), k=1, dim=-1)[1].squeeze().item()\n",
        "    probability = F.softmax(torch.from_numpy(logits), dim=-1).squeeze()[label].item()\n",
        "    return i2w[label]\n",
        "\n",
        "  df_test['pred'] = df_test['text'].apply(infer)\n",
        "  acc = accuracy_score(df_test['label'], df_test['pred'])\n",
        "  pre = precision_score(df_test['label'], df_test['pred'], average=\"macro\")\n",
        "  rec = recall_score(df_test['label'], df_test['pred'], average=\"macro\")\n",
        "  f1 = f1_score(df_test['label'], df_test['pred'], average=\"macro\")\n",
        "\n",
        "  return {\"accuracy\": acc,\n",
        "          \"precision\": pre,\n",
        "          \"recall\": rec,\n",
        "          \"f1\": f1}\n",
        "\n",
        "def eval_time_onnx(model):\n",
        "  start = time.time()\n",
        "  result = evaluate_onnx(model)\n",
        "  print(f\"\"\"\n",
        "        Accuracy:{result['accuracy']}\n",
        "        Precision:{result['precision']}\n",
        "        Recall: {result['recall']}\n",
        "        F1-Score: {result['f1']}\n",
        "        \"\"\")\n",
        "  end = time.time()\n",
        "  print(f\"Evaluation time: {end-start}\\n\")"
      ],
      "metadata": {
        "id": "k1P5bNxJ8CtI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_onnx = InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
        "quantized_onnx = InferenceSession(quantize_onnx_path, providers=[\"CPUExecutionProvider\"])"
      ],
      "metadata": {
        "id": "4iIWSBqH9M8j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Full Model ONNX\")\n",
        "eval_time_onnx(full_onnx)\n",
        "print(\"Quantized Model ONNX\")\n",
        "eval_time_onnx(quantized_onnx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bOhm3nR9Kq9",
        "outputId": "5fff4e03-d9a2-46f5-8b72-244f7395cd21"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Model ONNX\n",
            "\n",
            "        Accuracy:0.916\n",
            "        Precision:0.915580183764327\n",
            "        Recall: 0.875811280223045\n",
            "        F1-Score: 0.8905121652109605\n",
            "        \n",
            "Evaluation time: 69.74408197402954\n",
            "\n",
            "Quantized Model ONNX\n",
            "\n",
            "        Accuracy:0.912\n",
            "        Precision:0.9090679170218813\n",
            "        Recall: 0.8704208373326021\n",
            "        F1-Score: 0.8846322352346448\n",
            "        \n",
            "Evaluation time: 39.85102200508118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "### Model Size\n",
        "PyTorch quantizes torch.nn.Linear modules only and reduce the model from 474 MB to 230 MB. ONNXRuntime quantizes not only Linear(MatMul), but also the embedding layer. It achieves almost the ideal model size reduction with quantization."
      ],
      "metadata": {
        "id": "PxYix8mIM6hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Engine | Full Precision(MB) | Quantized(MB) |\n",
        "| --- | --- | --- |\n",
        "| PyTorch | 474.8 | 230.1 |\n",
        "| ONNX | 474.7 | 119.1 |"
      ],
      "metadata": {
        "id": "ZxMOhXdG-IoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy and F1-score"
      ],
      "metadata": {
        "id": "dzaqd4OHNPkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantized model of PyTorch and ONNX achieves similar result in accuracy and F1 score despite the later one has smaller size."
      ],
      "metadata": {
        "id": "Csd-1YcsNWFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| Metrics | Full Size | PyTorch Quantization | ORT Quantization |\n",
        "| --- | --- | --- | --- |\n",
        "| Accuracy | 0.916 | 0.912 | 0.912 |\n",
        "| F1 score | 0.890 | 0.885 | 0.884 |\n",
        "| Precision | 0.915 | 0.906 | 0.909 |\n",
        "| Recall | 0.875 | 0.872 | 0.870 |"
      ],
      "metadata": {
        "id": "FTvWCfndNK70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference Time\n",
        "It is shown that the quantized model of ONNX achieves the fastest inference time."
      ],
      "metadata": {
        "id": "vvFZNMoONlUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark(f, name=\"\"):\n",
        "    # warmup\n",
        "    for _ in range(10):\n",
        "        f()\n",
        "    seconds_per_iter = timeit.timeit(f, number=100) / 100\n",
        "    print(\n",
        "        f\"{name}:\",\n",
        "        f\"{seconds_per_iter * 1000:.3f} ms\",\n",
        "    )\n",
        "\n",
        "    return seconds_per_iter * 1000"
      ],
      "metadata": {
        "id": "tRN8-q2l_TVB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(indobert_path)\n",
        "inputs = tokenizer.encode(text)\n",
        "inputs = torch.LongTensor(inputs).view(1, -1).to(\"cpu\")\n",
        "\n",
        "inputs_onnx = tokenizer([text])\n",
        "inputs_onnx = dict(\n",
        "    input_ids=np.array(inputs_onnx[\"input_ids\"]).astype(\"int64\"),\n",
        "    attention_mask=np.array(inputs_onnx[\"attention_mask\"]).astype(\"int64\"),\n",
        "    token_type_ids=np.array(inputs_onnx[\"token_type_ids\"]).astype(\"int64\")\n",
        ")"
      ],
      "metadata": {
        "id": "GWoViI_Q_v5h"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speed_full_pt = benchmark(lambda: model(inputs), \"Full\")\n",
        "speed_quant_pt = benchmark(lambda: quantized_model(inputs), \"Quantized\")\n",
        "speed_quant_onnx = benchmark(lambda: full_onnx.run(None, input_feed=inputs_onnx), \"Full ONNX\")\n",
        "speed_full_onnx = benchmark(lambda: quantized_onnx.run(None, input_feed=inputs_onnx), \"Quantized ONNX\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ8SWVSB_xm6",
        "outputId": "ff8a93bf-5eb0-49ab-acfb-a69f7d7d5629"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full: 213.455 ms\n",
            "Quantized: 59.982 ms\n",
            "Full ONNX: 69.804 ms\n",
            "Quantized ONNX: 48.724 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing with PyTorch full precision, PyTorch quantization achieves ~3.5x speedup, and ORT quantization achieves ~1.4x speedup. ORT quantization can achieve ~4.3x speedup, comparing with PyTorch quantization."
      ],
      "metadata": {
        "id": "DMEB6YyVNz2r"
      }
    }
  ]
}